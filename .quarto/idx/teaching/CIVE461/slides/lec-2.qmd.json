{"title":"Lecture 2 - Statistical Modeling","markdown":{"yaml":{"title":"Lecture 2 - Statistical Modeling","subtitle":"CIVE 461/861: Urban Transportation Planning","footer":"[CIVE461 Home](/teaching/CIVE461/)","logo":"../images/logo.png","editor":"visual","format":{"revealjs":{"theme":"simple","transition":"fade","chalkboard":true,"slide-number":true,"multiplex":true}},"execute":{"freeze":"auto"}},"headingText":"Outline","containsRefs":false,"markdown":"\n\n\n::: incremental\n1.  What is statistics\n2.  What are key statistical measures to do transportation planning analysis\n3.  How to do regression models\n4.  Why statistics is really hard\n:::\n\n# Statistics Fundamentals\n\n## What is Statistics?\n\n> Study of methods and techniques to **summarize** & **interpret** data\n\n## Descriptive Statistics\n\n::: incremental\n-   Relative standing\n-   Central tendency\n-   Variability\n-   Association\n:::\n\n## Measures of Relative Standing {.smaller}\n\n-   **Quartiles** are percentage points that separate data into quarters\n    -   E.g., 25th percentile means 25% of data lie below this value (often referred to as *first quartile*)\n-   **Interquartile range** is difference between first quartile (25th percentile) and third quartile (75th percentile)\n-   50th percentile often referred to as *median*\n\n## Measures of Central Tendency {.smaller}\n\n-   **Median** often more useful than arithmetic **mean** (average) because mean **biased** by outliers\n-   Interquartile range similarly resident to outliers relative to **range**, which include minimum & maximum values\n\n## Measures of Variability {.smaller}\n\n::: incremental\n-   **Variance** & **standard deviation** more useful than **range** because use information from all observations\n-   Sample variance given by $$ s^2 = \\frac{\\sum_i(x_i-\\bar{x})^2}{n-1}$$\n-   **Why n-1?** By using sample mean, lose one degree of freedom - i.e., $n-1$ independent observations\n-   For small samples, tendency to underestimate standard deviation\n    -   $n-1$ approaches n as sample size grows\n:::\n\n## Measures of Association {.smaller}\n\n-   **Covariance** & **correlation** are measures of association between two variables\n-   Correlation a normalization of covariance to lie within interval \\[-1, 1\\]\n-   **Note**: possible for two variables with zero correlation to be nonlinearly related\n\n::::: columns\n::: {.column width=\"50%\"}\n![](../images/lecture2/rho_gt_0.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n![](../images/lecture2/rho_lt_0.png){fig-align=\"center\"}\n:::\n:::::\n\n## Measures of Association {.smaller}\n\n$$COV_p(X,Y) = \\frac{\\sum_i (x_i-\\mu_X)(y_i-\\mu_Y)}{N}$$ $$COV_s(X,Y) = \\frac{\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}$$ Pearson product-moment correlation parameter $$\\rho = \\frac{COV_p(X,Y)}{\\sigma_X \\sigma_Y}$$ $$r = \\frac{COV_s(X,Y)}{s_X s_Y}$$\n\n## Properties of Estimators {.smaller}\n\n. . .\n\n**Unbiasedness** means the expected value (mean) of estimator equals the associated population value\n\n![](../images/lecture2/unbiased.png){fig-align=\"center\"}\n\n. . .\n\n**Efficiency** is a relative measure of variance. An estimator with a smaller variance is said to be more *efficient*\n\n![](../images/lecture2/efficiency.png){fig-align=\"center\"}\n\n## Properties of Estimators {.smaller}\n\n. . .\n\n**Consistency** exists if the probability of being close to the true parameter value increases with increasing sample size\n\n![](../images/lecture2/consistency.png){fig-align=\"center\"}\n\n## Confidence Intervals\n\n::: incremental\n-   Interval estimates are based on *frequentist statistical theory*\n-   They say nothing about the location of the *true* parameter value\n-   They are a measure of how **likely** it is for **samples** taken from the **same population** to have their parameter values lie in that interval\n-   E.g., a 95% confidence interval of \\[1.15,2.34\\] means that 95 out of 100 samples will have the parameter value of interest in the range 1.15 to 2.34\n:::\n\n## Hypothesis Testing {.smaller}\n\n::: incremental\n-   Used to measure whether a difference in parameter values is likely to have arisen **by chance** or whether **some other factor** is responsible for the difference\n-   Statistical distributions used in hypothesis testing to estimate probabilities of observing the sample data, given an assumption about what *should have* occurred\n-   If observed results are extremely unlikely to have occurred by chance given assumed conditions, then assumed conditions are considered **unlikely**\n-   P(data‚îÇtrue null hypothesis) = Probability of observing the sample data conditional upon a true null hypothesis ‚Äì **NOT** probability of null hypothesis being true\n-   Often test parameter value equal to zero\n    -   **Problem:** often expect an effect and more interested in **variation in effect**\n:::\n\n. . .\n\n$$Z^* = \\frac{(\\bar{X_1}-\\bar{X_2})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}$$\n\n## Data Display Methods {.smaller}\n\n. . .\n\n**Histogram** useful when data naturally grouped\n\n![](../images/lecture2/histogram.png){fig-align=\"center\"}\n\n. . .\n\n**Box plot** (or box and whisker plot)\n\n![](../images/lecture2/box_plot.png){fig-align=\"center\"}\n\n# Basic Regression\n\n## Regression Model Uses {.smaller}\n\n::: incremental\n-   **Prediction** - Modeling existing observations or forecasting new data\n    -   Outcome variable(s) can be\n        -   **Continuous** like vote share in an election or future product sales\n        -   **Discrete** like individual voting decisions or victory in a sporting event\n-   **Explore Association** - Summarizing how well one variable, or set of variables, predicts outcomes\n    -   E.g., identifying risk factors for a disease or attitudes that predict voting\n:::\n\n## Poll\n\n::: {style=\"position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;\"}\n<iframe sandbox=\"allow-scripts allow-same-origin allow-presentation\" allowfullscreen=\"true\" allowtransparency=\"true\" frameborder=\"0\" height=\"315\" src=\"https://www.mentimeter.com/app/presentation/alf9arqzebexh9jdrkry69dwkgd8nydp/embed\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" width=\"420\">\n\n</iframe>\n:::\n\n## Regression Model Uses {.smaller}\n\n::: incremental\n-   **Extrapolation** - Adjusting for known differences between sample and population of interest\n    -   E.g., sample data from self-selecting schools to make conclusions about all schools in state\n-   **Causal inference** - Estimating treatment effects\n    -   E.g., exposure to a pollutant and health outcomes\n:::\n\n## Linear Regression {.smaller}\n\nConsider experiment where we have values of a certain variable $Y={Y_ùëñ}$ that takes different values based on the values of another variable $X$. If the process is **not deterministic**, we will observe different $Y_i$ for the same $X_i$\n\nLet‚Äôs call $f_i(Y|X)$ the **probability distribution** for $Y_i$ for a given $X_i$; this means we could have a different function $f_i$ for each $X$\n\n![](../images/lecture2/linear_regression_1.png){fig-align=\"center\"}\n\n## Linear Regression {.smaller}\n\nHowever, such a general case is **intractable**; to make it more manageable, certain hypotheses about population regularity assumed: - Probability distribution $f_i(Y|X)$ has **same variance** $\\sigma^2$ for all values of $X$ - Mean $\\mu_i = E(Y_i)$ forms a straight line known as the **true regression line** & given by $$ùê∏(ùëå)=a+bX_i$$ where $a$ and $b$ define the line & are estimated from sample data - Random variables $Y_i$ are statistically independent; i.e., a large value of $Y_1$ doesn‚Äôt necessarily make $Y_2$ large\n\n![](../images/lecture2/linear_regression_2.png){fig-align=\"center\"}\n\n## Linear Regression {.smaller}\n\nModel is often written as $Y_i = a + bX_i + e_i$ where $e_i$ measures error/disturbance in data and includes both **measurement** & **specification** error\n\n![](../images/lecture2/linear_regression_3.png){fig-align=\"center\"}\n\n## Fundamental Graph of Linear Regression\n\n![](../images/lecture2/fundamental_graph.png){fig-align=\"center\"}\n\n## Important Considerations for Linear Regression {.smaller}\n\n::: incremental\n-   Important to distinguish between **errors** $e_i$, which are unknown & associated with true regression line and **differences** $\\epsilon_i$ between observed $Y_i$ & fitted $\\hat{Y}$\n-   **Least squares estimation (LSE)** - most common line-fitting method, results from the minimization of $\\sqrt{\\epsilon_i}$\n-   A **change of variables** can help to understand properties of the linear regression model $$x_i = X_i - \\bar{X}$$ where $\\bar{X}$ is the mean of $\\mathbf{X}$\n:::\n\n## Important Considerations for Linear Regression {.smaller}\n\n::: incremental\n-   Previous regression lines keep their slopes ($b$ and $\\hat{b}$) but change their intercepts ($a$ and $\\hat{a}$)\n-   Change is useful because new variable $x$ has property that $\\sum_i x_i = 0$\n-   Under this transformation, LSE are given by $\\hat{a} = \\hat{Y}$ so fitted line goes through the center of gravity ($\\bar{X}, \\bar{Y}$) of the sample and $$\\hat{b} = \\sum_i \\frac{x_i Y_i}{x_i^2}$$\n:::\n\n## Important Considerations for Linear Regression {.smaller}\n\nEstimator properties given by\n\n\\begin{array}\n\nEE(\\hat{a}) = a & Var(\\bar{a}) = \\frac{\\sigma^2}{n} \\\\\nE(\\hat{b}) = b & Var(\\bar{b}) = \\frac{\\sigma^2}{\\sum_i x_i^2} \\\\\n\n\\end{array}\n\n## Interesting Experimental Design Point from Previous {.smaller}\n\n-   Considering $Var(\\hat{a}) = \\frac{\\sigma^2}{n}$ & $Var(\\hat{b}) = \\frac{\\sigma^2}{\\sum_i x_i^2}$\n    -   Variance of both estimators **decreases** with sample size\n    -   Variance of $\\hat{b}$ tends to grow with closer together $x_i$ & $\\hat{b}$ becomes **unreliable estimator**\n    -   Increased data spread (sampling points from across expected range) tends to decrease $Var(\\hat{b})$\n\n![](../images/lecture2/experimental_design.png){fig-align=\"center\" width=\"50%\"}\n\n## Properties of Regression Estimators {.smaller}\n\n::: incremental\n-   If $E(e|\\mathbf{X}) = 0$, LSE have some desirable properties\n    -   Estimators are **unbiased** (i.e., expected values are equal to true values for $a$ and $b$)\n    -   Estimators are **consistent** (i.e., approach the true values with increasing sample size)\n    -   Assumption easily violated if relevant variable omitted from model correlated with observed $\\mathbf{X}$\n    -   E.g., trip generation\n        -   Depends on household income and number of vehicles, which are **positively correlated** variables since household more likely to own a vehicle as income grows\n        -   If number of vehicles omitted, LSE of income will include both income and number of vehicles effects; therefore parameter value will be larger than true value\n:::\n\n## Properties of Regression Estimators\n\nIf prior conditions hold, LSE are not only **consistent** & **unbiased**, but also **best** (most efficient/smallest variance) among possible linear unbiased estimators (BLUE) ‚Äì known as **Gauss-Markov theorem**\n\n## Hypothesis Testing {.smaller}\n\n-   Need to know distribution of $\\hat{b}$, which requires strong assumption that variables $Y$ are distributed normal\n-   For LSE, is BLUE and also BUE (best unbiased estimators) among all linear and non-linear estimators\n    -   Strong assumption but as sample size grows holds true no matter the true distribution due to **Law of Large Numbers**\n    -   Since $\\hat{b}$ are linear combinations of $Y$, they are distributed $N=(b,\\frac{\\sigma^2}{\\sum_i x_i^2}$\n    -   Can use the normal standardization to obtain a test statistic distributed standard normal N(0, 1) $$z=\\frac{\\hat{b}-b}{\\sigma/\\sqrt{\\sum_i x_i^2}}$$\n-   Do not know $\\sigma^2$ but can use the residual variance $s^2$\n\n## Hypothesis Testing {.smaller}\n\n-   Substitution of $s$ for $\\sigma$ means standardized $\\hat{b}$ is distributed Student t with (n-2) degrees of freedom $$t=\\frac{\\hat{b}-b}{s/\\sqrt{\\sum_i x_i^2}}$$\n-   Denominator of above is usually called **standard error**. Typical null hypothesis is that $b=0$\n-   standard error given by $$s^2= \\frac{\\sum_i (Y_i - \\hat{Y}_i)^2}{n-k}$$\n-   Where $k$ is the number of parameters in the model. If n \\>\\> k, t-statistics approximates a z-statistic\n\n<!-- Lower variance for larger sample size with fixed individual observation random variable assumption because individual variance is dominated by description of inter-observation variance described by model. -->\n\n## Hypothesis Testing Interpretation\n\nRejection region for $\\alpha = 5%$ for variable with assumed (known) sign ‚Äì e.g., income effect on vehicle ownership\n\n![](../images/lecture2/hypothesis_testing.png){fig-align=\"center\" width=\"50%\"}\n\n## Coefficient of Determination\n\nMeasures the percent of total variation from the mean explained by the model\n\n$$R^2=\\frac{\\sum_i (\\hat{Y}_i-\\bar{Y})^2}{\\sum_i (Y_i-\\bar{Y})^2}$$\n\n## Multiple Regression {.smaller}\n\n-   Often interested in cases where we have more than one explanatory variable\n-   Some additional complications with multiple regression\n    -   **Multicollinearity** occurs when linear relationship exists between explanatory variables\n    -   How many regressors to include in model?\n        -   Are there **strong theoretical reasons** to include a variable, or is it important for **policy analysis**?\n        -   Is the estimated **sign** of the parameter consistent with theory or intuition & is the parameter **statistically significant** (i.e., $H_0$ rejected in the t-test)\n        -   **Simple models** are preferred to **complex models**. If removing a variable has minimal effect on fit, can (and probably should) by removed from model\n\n## Multiple Regression {.smaller}\n\n-   **Coefficient of determination:** Including additional variables always increases fit, so should use adjusted $R^2$ $$R_{adj}^2 = R^2 - \\frac{k}{n-1}\\frac{n-1}{n-k-1}$$\n-   Where $n$ is sample size & $k$ is number of variables in model\n-   If interested in differences between models with restrictions on included variables, can use F-test $$\\hat{F} = \\frac{(SSR_R - SSR_U)(n-k)}{r SSR_U} ~ F_{r,n-k}$$\n-   Follows F-distribution with r & n-k degrees of freedom\n-   **Intuition:** if restrictions are valid, $SSR_R$ should be similar to $SSR_U$ & statistic will be near zero\n\n## Building, Interpreting, & Checking Regression Models\n\n```{dot}\ndigraph model_flow {\n\tlayout = neato\n\tN1 [fontsize=6, shape=circle, fontcolor = white, color=red, margin=\"0.01,0.01\", style=filled, label = \"Build linear \\n regression model \\n y = a + bx + error\"];\n  N2 [fontsize=6, shape=circle, fontcolor = white, color=green, margin=\"0.01,0.01\", style=filled, label = \"Expand model \\n with additional \\n predictors, \\n interactions, & \\n transformations\"];\n  N3 [fontsize=6, shape=circle, fontcolor = white, color=purple, margin=\"0.01,0.01\", style=filled, label = \"Model fitting ‚Äì \\n programming \\n & algorithms\"];\n  N4 [fontsize=6, shape=circle, fontcolor = white, color=blue, margin=\"0.01,0.01\", style=filled, label = \"Model \\n understanding ‚Äì \\n graphics, more \\n programming, & \\n investigation\"];\n  N5 [fontsize=6, shape=circle, color=orange, margin=\"0.01,0.01\", style=filled, label = \"Criticism ‚Äì Flaws, \\n assumptions, & \\n improvements\"];\n\n  N1 -> N2 [color=red];\n  N2 -> N3 [color=green];\n  N3 -> N4 [color=purple];\n  N4 -> N5 [color=blue];\n  N5 -> N1 [color=orange];\n}\n```\n\n````{=html}\n<!-- \n```{mermaid}\ngraph LR\n    A((\"Build linear \n  regression model\n  y = a + bx + error\")) -- B((\"Expand model \n  with additional \n  predictors, interactions, \n  & transformations\"))\n  B((\"Expand model \n  with additional \n  predictors, interactions, \n  & transformations\"))--\n  C((\"Model fitting ‚Äì\n  programming\n  & algorithms\"))\n  C((\"Model fitting ‚Äì\n  programming\n  & algorithms\"))--\n  D((\"Model\n  understanding ‚Äì\n  graphics, more\n  programming, &\n  investigation\"))\n  D((\"Model fitting ‚Äì\n  programming\n  & algorithms\"))--\n  E((\"Criticism ‚Äì Flaws,\n  assumptions, &\n  improvements\"))\n  E--A\n  style A fill:#C0504D,stroke:#333,stroke-width:4px,color:#fff\n  style B fill:#9BBB59,stroke:#333,stroke-width:4px,color:#fff\n  style C fill:#8064A2,stroke:#333,stroke-width:4px,color:#fff\n  style D fill:#4BACC6,stroke:#333,stroke-width:4px,color:#fff\n  style E fill:#F79646,stroke:#333,stroke-width:4px,color:#fff\n``` -->\n````\n\n## Power Law & Exponential Growth/Decline {.smaller}\n\n-   Line $y = a + bx$ can be used to represent a more general class of relationships by allowing logarithmic transformations\n-   $ln(y) = a + bx$ is exponential growth (if $b > 0$) and decline (if $b < 0$) -\\> $y = A e^{bx}$\n\n**Power law:**\n\nLet y be the area of a square and x be its perimeter. Then $y = (x/4)^2$, and we can take the ln of both sides to get $ln(y) = 2(ln(x) ‚Äì ln(4)) = -2.8 + 2 ln(x)$\n\n## Power Law & Exponential Growth/Decline {.smaller}\n\n**Exponential growth:**\n\nSuppose the world population starts at 1.5 billion in the year 1900 and increases exponentially, doubling every 50 years. We can write this as $y = A \\times 2^{(x-1900)/50}$, where $A = 1.5 \\times 10^9$\n\nEquivalently, $y = A \\times e^{0.014(x-1900)}$ meaning that y increases by 1.014 per year, or 1.15 per 10 years, or 4.0 per hundred years ($e^{1.4} = 4.0 = 400\\%$ increase)\n\n$ln(y) = a + b ln(x)$ represents power law growth (if $b > 0$) or decline (if $b < 0$) -\\> $y = A x^b$\n\n## Problems with p-values {.smaller}\n\n[**Type 1 error:**]{style=\"color:#4F81BD\"} probability of falsely rejecting a true null hypothesis (false positive)\n\n[**Type 2 error:**]{style=\"color:#C00000\"} probability of not rejecting a false null hypothesis (false negative)\n\n![](../images/lecture2/p_value.png)\n\n![](../images/lecture2/null_hypothesis.png){.absolute .fragment .fade-in bottom=\"400\" left=\"150\" width=\"25%\"}\n\n![](../images/lecture2/alt_hypothesis.png){.absolute .fragment .fade-in bottom=\"400\" left=\"450\" width=\"25%\"}\n\n![](../images/lecture2/blue_wedge.png){.absolute .fragment .fade-in bottom=\"120\" left=\"460\" width=\"10%\"}\n\n![](../images/lecture2/red_wedge.png){.absolute .fragment .fade-in bottom=\"120\" left=\"250\" width=\"20%\"}\n\n![](../images/lecture2/sample_size.png){.absolute .fragment .fade-in bottom=\"300\" left=\"300\" width=\"20%\"}\n\n![](../images/lecture2/red_arrow.png){.absolute .fragment .fade-in bottom=\"250\" left=\"300\" width=\"20%\"}\n\n## Poll\n\n::: {style=\"position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;\"}\n<iframe sandbox=\"allow-scripts allow-same-origin allow-presentation\" allowfullscreen=\"true\" allowtransparency=\"true\" frameborder=\"0\" height=\"315\" src=\"https://www.mentimeter.com/app/presentation/alf9arqzebexh9jdrkry69dwkgd8nydp/embed\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" width=\"420\">\n\n</iframe>\n:::\n\n## Problems with p-values {.smaller}\n\nAnswer: Depends but general safe answer...NO!\n\n-   E.g., A change in the gas price will produce some changes in travel behavior. How do these changes vary across people and contexts?\n-   A medical intervention will work differently for different people\n-   A political advertisement will change the opinion of some people but not others\n-   In many cases, there is no interest in a null effect\n-   When a hypothesis is rejected (i.e., a study is a success), researchers and practitioners make decisions based on point estimates of the magnitude and sign\n    -   We are interested in the magnitude (M) and sign (S) conditional on an effect being statistically significant\n\n## Statistically Significance $\\neq$ Practical Importance {.smaller}\n\n::: incremental\n-   An effect can be non-random (i.e., statistically significant) but not practically impactful\n-   E.g., say we find that for every 10-cent increase in the price per gallon for gasoline there is a 0.001% decrease in vehicle miles traveled. Do we care?\n    -   Approximate actual elasticity values: -0.0216 (short-run) and -0.1066 (long-run)\n    -   Meaning a 1% increase in the price of gas produces a 0.026% decrease in VMT\n    -   If gas price rises from \\$3.00 per gallon to \\$3.10 per gallon\n        -   3.3% increase in price -\\> 0.09% short-run decrease in VMT\n        -   If we double the price -\\> 2.2% short-run decrease in VMT OR 10.7% in long-run\n:::\n\n## Not Significant Does Not Mean Zero Effect {.smaller}\n\n::: incremental\n-   E.g., A study of the effectiveness of heart stents for heart patients\n-   Treated group outperformed control group, but not statistically significantly so\n-   Observed average difference in treadmill time (treatment effect) was 16.6 seconds with standard error of 9.8 ‚Äì 95% confidence interval includes zero\n-   Net effect may be positive or negative ‚Äì it is unclear!\n:::\n\n## Differences B/W *Significant* & *Not Significant* NOT Statistically Significant {.smaller}\n\n::: incremental\n-   Moving from 0.051 p-value to 0.049 p-value is not hard\n-   More important:\n    -   Large differences in significance level may not mean large differences in underlying variable\n    -   E.g., Two independent studies with effect estimates and standard errors of 25 ¬± 10 and 10 ¬± 10\n        -   First study is significant at 0.01 level (25/10 = 2.5)\n        -   Second study is not significant (10/10 = 1.0)\n        -   Is there a large difference between the two effect estimates?\n        -   Difference is 15 with a standard error of $\\sqrt{(10^2+10^2)}=14$ meaning 15/14 = 1.07!\n:::\n\n## Garden of Forking Paths {.smaller}\n\n-   When many ways to select, exclude, & analyze data, not difficult to attain a low p-value (even in absence of true effect)\n-   More than ‚Äúfile drawer effect‚Äù ‚Äì not publishing non-significant results\n-   ‚ÄúDegrees of freedom‚Äù available to analyst when coding & analyzing data\n-   Even if only one analysis done, there are many others that could be done that would result in non-significant results\n\n![](../images/lecture2/forking_fish.jpg){fig-align=\"center\"}\n\n```{=html}\n<!-- Neuroscientist Craig Bennett purchased a whole Atlantic salmon, took it to a lab at Dartmouth, and put it into an fMRI machine used to study the brain. The beautiful fish was to be the lab's test object as they worked out some new methods.\n\nSo, as the fish sat in the scanner, they showed it \"a series of photographs depicting human individuals in social situations.\" To maintain the rigor of the protocol (and perhaps because it was hilarious), the salmon, just like a human test subject, \"was asked to determine what emotion the individual in the photo must have been experiencing.‚Äú\n\nThe salmon, as¬†Bennett's poster on the test dryly notes, \"was not alive at the time of scanning.\"\n\nanalyzing the voxel (think: 3-D or \"volumetric\" pixel) data, the voxels representing the area where the salmon's tiny brain sat showed evidence of activity. In the fMRI scan, it looked like the dead salmon was¬†actually thinking¬†about the pictures it had been shown.\n\n\"By complete, random chance, we found some voxels that were significant that just happened to be in the fish's brain,\" Bennett said. \"And if I were a ridiculous researcher, I'd say, 'A dead salmon perceiving humans can tell their emotional state.'\" -->\n```\n\n## Example: Sports Viewing & Political Attitudes {.smaller}\n\nConsider the below article excerpt (based on an article in a leading psychology journal):\n\nMany people watch UNL Cornhusker football games. Whereas research finds that Cornhuskers viewing influences Nebraskans‚Äô mating preferences, we propose that it might also change their political and religious views. Building on theory suggesting that political and religious orientation are linked to viewing preference, we test how football season influenced Nebraskans‚Äô politics, religiosity, and voting in the 2012 U.S. presidential election. In two studies with large and diverse samples, whether it was football season had drastically different effects on single versus married Nebraskans. Football season led single Nebraskans to become more liberal, less religious, and more likely to vote for Barack Obama. In contrast, football led married Nebraskans to become more conservative, more religious, and more likely to vote for Mitt Romney. In addition, football-induced changes in political orientation mediated Nebraskans‚Äô voting behavior. Overall, the football season not only influences Nebraskans‚Äô politics, but appears to do so differently for single versus married Nebraskans.\n\n## Example: Sports Viewing & Political Attitudes\n\n::: incremental\n-   Find **40%** of Nebraskans who watch football supported Romney in football season vs. **23%** in non-football season (Type-M error)\n\n-   Implausible! Research finds **minimal changes** in vote preferences over election cycle\n\n-   What is the dividing line between **single** and **married**?\n\n-   **Differential response rate?** Maybe liberal or conservative Nebraskans are more or less likely to participate in a survey depending on if it is football season\n:::\n\n## How to Move Beyond Hypothesis Testing\n\n::: incremental\n1.  **Analyze all data** - Better to anticipate critism than hide data\n2.  **Present all comparisons** - Rather than statistically significant comparisons only\n3.  **Make data public** - If a topic is worth studying, you should want others to be able to quickly progress without repeating your work\n4.  **Accept uncertainty & embrace variation** - No answer is perfect\n:::","srcMarkdownNoYaml":"\n\n## Outline\n\n::: incremental\n1.  What is statistics\n2.  What are key statistical measures to do transportation planning analysis\n3.  How to do regression models\n4.  Why statistics is really hard\n:::\n\n# Statistics Fundamentals\n\n## What is Statistics?\n\n> Study of methods and techniques to **summarize** & **interpret** data\n\n## Descriptive Statistics\n\n::: incremental\n-   Relative standing\n-   Central tendency\n-   Variability\n-   Association\n:::\n\n## Measures of Relative Standing {.smaller}\n\n-   **Quartiles** are percentage points that separate data into quarters\n    -   E.g., 25th percentile means 25% of data lie below this value (often referred to as *first quartile*)\n-   **Interquartile range** is difference between first quartile (25th percentile) and third quartile (75th percentile)\n-   50th percentile often referred to as *median*\n\n## Measures of Central Tendency {.smaller}\n\n-   **Median** often more useful than arithmetic **mean** (average) because mean **biased** by outliers\n-   Interquartile range similarly resident to outliers relative to **range**, which include minimum & maximum values\n\n## Measures of Variability {.smaller}\n\n::: incremental\n-   **Variance** & **standard deviation** more useful than **range** because use information from all observations\n-   Sample variance given by $$ s^2 = \\frac{\\sum_i(x_i-\\bar{x})^2}{n-1}$$\n-   **Why n-1?** By using sample mean, lose one degree of freedom - i.e., $n-1$ independent observations\n-   For small samples, tendency to underestimate standard deviation\n    -   $n-1$ approaches n as sample size grows\n:::\n\n## Measures of Association {.smaller}\n\n-   **Covariance** & **correlation** are measures of association between two variables\n-   Correlation a normalization of covariance to lie within interval \\[-1, 1\\]\n-   **Note**: possible for two variables with zero correlation to be nonlinearly related\n\n::::: columns\n::: {.column width=\"50%\"}\n![](../images/lecture2/rho_gt_0.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n![](../images/lecture2/rho_lt_0.png){fig-align=\"center\"}\n:::\n:::::\n\n## Measures of Association {.smaller}\n\n$$COV_p(X,Y) = \\frac{\\sum_i (x_i-\\mu_X)(y_i-\\mu_Y)}{N}$$ $$COV_s(X,Y) = \\frac{\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}$$ Pearson product-moment correlation parameter $$\\rho = \\frac{COV_p(X,Y)}{\\sigma_X \\sigma_Y}$$ $$r = \\frac{COV_s(X,Y)}{s_X s_Y}$$\n\n## Properties of Estimators {.smaller}\n\n. . .\n\n**Unbiasedness** means the expected value (mean) of estimator equals the associated population value\n\n![](../images/lecture2/unbiased.png){fig-align=\"center\"}\n\n. . .\n\n**Efficiency** is a relative measure of variance. An estimator with a smaller variance is said to be more *efficient*\n\n![](../images/lecture2/efficiency.png){fig-align=\"center\"}\n\n## Properties of Estimators {.smaller}\n\n. . .\n\n**Consistency** exists if the probability of being close to the true parameter value increases with increasing sample size\n\n![](../images/lecture2/consistency.png){fig-align=\"center\"}\n\n## Confidence Intervals\n\n::: incremental\n-   Interval estimates are based on *frequentist statistical theory*\n-   They say nothing about the location of the *true* parameter value\n-   They are a measure of how **likely** it is for **samples** taken from the **same population** to have their parameter values lie in that interval\n-   E.g., a 95% confidence interval of \\[1.15,2.34\\] means that 95 out of 100 samples will have the parameter value of interest in the range 1.15 to 2.34\n:::\n\n## Hypothesis Testing {.smaller}\n\n::: incremental\n-   Used to measure whether a difference in parameter values is likely to have arisen **by chance** or whether **some other factor** is responsible for the difference\n-   Statistical distributions used in hypothesis testing to estimate probabilities of observing the sample data, given an assumption about what *should have* occurred\n-   If observed results are extremely unlikely to have occurred by chance given assumed conditions, then assumed conditions are considered **unlikely**\n-   P(data‚îÇtrue null hypothesis) = Probability of observing the sample data conditional upon a true null hypothesis ‚Äì **NOT** probability of null hypothesis being true\n-   Often test parameter value equal to zero\n    -   **Problem:** often expect an effect and more interested in **variation in effect**\n:::\n\n. . .\n\n$$Z^* = \\frac{(\\bar{X_1}-\\bar{X_2})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}$$\n\n## Data Display Methods {.smaller}\n\n. . .\n\n**Histogram** useful when data naturally grouped\n\n![](../images/lecture2/histogram.png){fig-align=\"center\"}\n\n. . .\n\n**Box plot** (or box and whisker plot)\n\n![](../images/lecture2/box_plot.png){fig-align=\"center\"}\n\n# Basic Regression\n\n## Regression Model Uses {.smaller}\n\n::: incremental\n-   **Prediction** - Modeling existing observations or forecasting new data\n    -   Outcome variable(s) can be\n        -   **Continuous** like vote share in an election or future product sales\n        -   **Discrete** like individual voting decisions or victory in a sporting event\n-   **Explore Association** - Summarizing how well one variable, or set of variables, predicts outcomes\n    -   E.g., identifying risk factors for a disease or attitudes that predict voting\n:::\n\n## Poll\n\n::: {style=\"position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;\"}\n<iframe sandbox=\"allow-scripts allow-same-origin allow-presentation\" allowfullscreen=\"true\" allowtransparency=\"true\" frameborder=\"0\" height=\"315\" src=\"https://www.mentimeter.com/app/presentation/alf9arqzebexh9jdrkry69dwkgd8nydp/embed\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" width=\"420\">\n\n</iframe>\n:::\n\n## Regression Model Uses {.smaller}\n\n::: incremental\n-   **Extrapolation** - Adjusting for known differences between sample and population of interest\n    -   E.g., sample data from self-selecting schools to make conclusions about all schools in state\n-   **Causal inference** - Estimating treatment effects\n    -   E.g., exposure to a pollutant and health outcomes\n:::\n\n## Linear Regression {.smaller}\n\nConsider experiment where we have values of a certain variable $Y={Y_ùëñ}$ that takes different values based on the values of another variable $X$. If the process is **not deterministic**, we will observe different $Y_i$ for the same $X_i$\n\nLet‚Äôs call $f_i(Y|X)$ the **probability distribution** for $Y_i$ for a given $X_i$; this means we could have a different function $f_i$ for each $X$\n\n![](../images/lecture2/linear_regression_1.png){fig-align=\"center\"}\n\n## Linear Regression {.smaller}\n\nHowever, such a general case is **intractable**; to make it more manageable, certain hypotheses about population regularity assumed: - Probability distribution $f_i(Y|X)$ has **same variance** $\\sigma^2$ for all values of $X$ - Mean $\\mu_i = E(Y_i)$ forms a straight line known as the **true regression line** & given by $$ùê∏(ùëå)=a+bX_i$$ where $a$ and $b$ define the line & are estimated from sample data - Random variables $Y_i$ are statistically independent; i.e., a large value of $Y_1$ doesn‚Äôt necessarily make $Y_2$ large\n\n![](../images/lecture2/linear_regression_2.png){fig-align=\"center\"}\n\n## Linear Regression {.smaller}\n\nModel is often written as $Y_i = a + bX_i + e_i$ where $e_i$ measures error/disturbance in data and includes both **measurement** & **specification** error\n\n![](../images/lecture2/linear_regression_3.png){fig-align=\"center\"}\n\n## Fundamental Graph of Linear Regression\n\n![](../images/lecture2/fundamental_graph.png){fig-align=\"center\"}\n\n## Important Considerations for Linear Regression {.smaller}\n\n::: incremental\n-   Important to distinguish between **errors** $e_i$, which are unknown & associated with true regression line and **differences** $\\epsilon_i$ between observed $Y_i$ & fitted $\\hat{Y}$\n-   **Least squares estimation (LSE)** - most common line-fitting method, results from the minimization of $\\sqrt{\\epsilon_i}$\n-   A **change of variables** can help to understand properties of the linear regression model $$x_i = X_i - \\bar{X}$$ where $\\bar{X}$ is the mean of $\\mathbf{X}$\n:::\n\n## Important Considerations for Linear Regression {.smaller}\n\n::: incremental\n-   Previous regression lines keep their slopes ($b$ and $\\hat{b}$) but change their intercepts ($a$ and $\\hat{a}$)\n-   Change is useful because new variable $x$ has property that $\\sum_i x_i = 0$\n-   Under this transformation, LSE are given by $\\hat{a} = \\hat{Y}$ so fitted line goes through the center of gravity ($\\bar{X}, \\bar{Y}$) of the sample and $$\\hat{b} = \\sum_i \\frac{x_i Y_i}{x_i^2}$$\n:::\n\n## Important Considerations for Linear Regression {.smaller}\n\nEstimator properties given by\n\n\\begin{array}\n\nEE(\\hat{a}) = a & Var(\\bar{a}) = \\frac{\\sigma^2}{n} \\\\\nE(\\hat{b}) = b & Var(\\bar{b}) = \\frac{\\sigma^2}{\\sum_i x_i^2} \\\\\n\n\\end{array}\n\n## Interesting Experimental Design Point from Previous {.smaller}\n\n-   Considering $Var(\\hat{a}) = \\frac{\\sigma^2}{n}$ & $Var(\\hat{b}) = \\frac{\\sigma^2}{\\sum_i x_i^2}$\n    -   Variance of both estimators **decreases** with sample size\n    -   Variance of $\\hat{b}$ tends to grow with closer together $x_i$ & $\\hat{b}$ becomes **unreliable estimator**\n    -   Increased data spread (sampling points from across expected range) tends to decrease $Var(\\hat{b})$\n\n![](../images/lecture2/experimental_design.png){fig-align=\"center\" width=\"50%\"}\n\n## Properties of Regression Estimators {.smaller}\n\n::: incremental\n-   If $E(e|\\mathbf{X}) = 0$, LSE have some desirable properties\n    -   Estimators are **unbiased** (i.e., expected values are equal to true values for $a$ and $b$)\n    -   Estimators are **consistent** (i.e., approach the true values with increasing sample size)\n    -   Assumption easily violated if relevant variable omitted from model correlated with observed $\\mathbf{X}$\n    -   E.g., trip generation\n        -   Depends on household income and number of vehicles, which are **positively correlated** variables since household more likely to own a vehicle as income grows\n        -   If number of vehicles omitted, LSE of income will include both income and number of vehicles effects; therefore parameter value will be larger than true value\n:::\n\n## Properties of Regression Estimators\n\nIf prior conditions hold, LSE are not only **consistent** & **unbiased**, but also **best** (most efficient/smallest variance) among possible linear unbiased estimators (BLUE) ‚Äì known as **Gauss-Markov theorem**\n\n## Hypothesis Testing {.smaller}\n\n-   Need to know distribution of $\\hat{b}$, which requires strong assumption that variables $Y$ are distributed normal\n-   For LSE, is BLUE and also BUE (best unbiased estimators) among all linear and non-linear estimators\n    -   Strong assumption but as sample size grows holds true no matter the true distribution due to **Law of Large Numbers**\n    -   Since $\\hat{b}$ are linear combinations of $Y$, they are distributed $N=(b,\\frac{\\sigma^2}{\\sum_i x_i^2}$\n    -   Can use the normal standardization to obtain a test statistic distributed standard normal N(0, 1) $$z=\\frac{\\hat{b}-b}{\\sigma/\\sqrt{\\sum_i x_i^2}}$$\n-   Do not know $\\sigma^2$ but can use the residual variance $s^2$\n\n## Hypothesis Testing {.smaller}\n\n-   Substitution of $s$ for $\\sigma$ means standardized $\\hat{b}$ is distributed Student t with (n-2) degrees of freedom $$t=\\frac{\\hat{b}-b}{s/\\sqrt{\\sum_i x_i^2}}$$\n-   Denominator of above is usually called **standard error**. Typical null hypothesis is that $b=0$\n-   standard error given by $$s^2= \\frac{\\sum_i (Y_i - \\hat{Y}_i)^2}{n-k}$$\n-   Where $k$ is the number of parameters in the model. If n \\>\\> k, t-statistics approximates a z-statistic\n\n<!-- Lower variance for larger sample size with fixed individual observation random variable assumption because individual variance is dominated by description of inter-observation variance described by model. -->\n\n## Hypothesis Testing Interpretation\n\nRejection region for $\\alpha = 5%$ for variable with assumed (known) sign ‚Äì e.g., income effect on vehicle ownership\n\n![](../images/lecture2/hypothesis_testing.png){fig-align=\"center\" width=\"50%\"}\n\n## Coefficient of Determination\n\nMeasures the percent of total variation from the mean explained by the model\n\n$$R^2=\\frac{\\sum_i (\\hat{Y}_i-\\bar{Y})^2}{\\sum_i (Y_i-\\bar{Y})^2}$$\n\n## Multiple Regression {.smaller}\n\n-   Often interested in cases where we have more than one explanatory variable\n-   Some additional complications with multiple regression\n    -   **Multicollinearity** occurs when linear relationship exists between explanatory variables\n    -   How many regressors to include in model?\n        -   Are there **strong theoretical reasons** to include a variable, or is it important for **policy analysis**?\n        -   Is the estimated **sign** of the parameter consistent with theory or intuition & is the parameter **statistically significant** (i.e., $H_0$ rejected in the t-test)\n        -   **Simple models** are preferred to **complex models**. If removing a variable has minimal effect on fit, can (and probably should) by removed from model\n\n## Multiple Regression {.smaller}\n\n-   **Coefficient of determination:** Including additional variables always increases fit, so should use adjusted $R^2$ $$R_{adj}^2 = R^2 - \\frac{k}{n-1}\\frac{n-1}{n-k-1}$$\n-   Where $n$ is sample size & $k$ is number of variables in model\n-   If interested in differences between models with restrictions on included variables, can use F-test $$\\hat{F} = \\frac{(SSR_R - SSR_U)(n-k)}{r SSR_U} ~ F_{r,n-k}$$\n-   Follows F-distribution with r & n-k degrees of freedom\n-   **Intuition:** if restrictions are valid, $SSR_R$ should be similar to $SSR_U$ & statistic will be near zero\n\n## Building, Interpreting, & Checking Regression Models\n\n```{dot}\ndigraph model_flow {\n\tlayout = neato\n\tN1 [fontsize=6, shape=circle, fontcolor = white, color=red, margin=\"0.01,0.01\", style=filled, label = \"Build linear \\n regression model \\n y = a + bx + error\"];\n  N2 [fontsize=6, shape=circle, fontcolor = white, color=green, margin=\"0.01,0.01\", style=filled, label = \"Expand model \\n with additional \\n predictors, \\n interactions, & \\n transformations\"];\n  N3 [fontsize=6, shape=circle, fontcolor = white, color=purple, margin=\"0.01,0.01\", style=filled, label = \"Model fitting ‚Äì \\n programming \\n & algorithms\"];\n  N4 [fontsize=6, shape=circle, fontcolor = white, color=blue, margin=\"0.01,0.01\", style=filled, label = \"Model \\n understanding ‚Äì \\n graphics, more \\n programming, & \\n investigation\"];\n  N5 [fontsize=6, shape=circle, color=orange, margin=\"0.01,0.01\", style=filled, label = \"Criticism ‚Äì Flaws, \\n assumptions, & \\n improvements\"];\n\n  N1 -> N2 [color=red];\n  N2 -> N3 [color=green];\n  N3 -> N4 [color=purple];\n  N4 -> N5 [color=blue];\n  N5 -> N1 [color=orange];\n}\n```\n\n````{=html}\n<!-- \n```{mermaid}\ngraph LR\n    A((\"Build linear \n  regression model\n  y = a + bx + error\")) -- B((\"Expand model \n  with additional \n  predictors, interactions, \n  & transformations\"))\n  B((\"Expand model \n  with additional \n  predictors, interactions, \n  & transformations\"))--\n  C((\"Model fitting ‚Äì\n  programming\n  & algorithms\"))\n  C((\"Model fitting ‚Äì\n  programming\n  & algorithms\"))--\n  D((\"Model\n  understanding ‚Äì\n  graphics, more\n  programming, &\n  investigation\"))\n  D((\"Model fitting ‚Äì\n  programming\n  & algorithms\"))--\n  E((\"Criticism ‚Äì Flaws,\n  assumptions, &\n  improvements\"))\n  E--A\n  style A fill:#C0504D,stroke:#333,stroke-width:4px,color:#fff\n  style B fill:#9BBB59,stroke:#333,stroke-width:4px,color:#fff\n  style C fill:#8064A2,stroke:#333,stroke-width:4px,color:#fff\n  style D fill:#4BACC6,stroke:#333,stroke-width:4px,color:#fff\n  style E fill:#F79646,stroke:#333,stroke-width:4px,color:#fff\n``` -->\n````\n\n## Power Law & Exponential Growth/Decline {.smaller}\n\n-   Line $y = a + bx$ can be used to represent a more general class of relationships by allowing logarithmic transformations\n-   $ln(y) = a + bx$ is exponential growth (if $b > 0$) and decline (if $b < 0$) -\\> $y = A e^{bx}$\n\n**Power law:**\n\nLet y be the area of a square and x be its perimeter. Then $y = (x/4)^2$, and we can take the ln of both sides to get $ln(y) = 2(ln(x) ‚Äì ln(4)) = -2.8 + 2 ln(x)$\n\n## Power Law & Exponential Growth/Decline {.smaller}\n\n**Exponential growth:**\n\nSuppose the world population starts at 1.5 billion in the year 1900 and increases exponentially, doubling every 50 years. We can write this as $y = A \\times 2^{(x-1900)/50}$, where $A = 1.5 \\times 10^9$\n\nEquivalently, $y = A \\times e^{0.014(x-1900)}$ meaning that y increases by 1.014 per year, or 1.15 per 10 years, or 4.0 per hundred years ($e^{1.4} = 4.0 = 400\\%$ increase)\n\n$ln(y) = a + b ln(x)$ represents power law growth (if $b > 0$) or decline (if $b < 0$) -\\> $y = A x^b$\n\n## Problems with p-values {.smaller}\n\n[**Type 1 error:**]{style=\"color:#4F81BD\"} probability of falsely rejecting a true null hypothesis (false positive)\n\n[**Type 2 error:**]{style=\"color:#C00000\"} probability of not rejecting a false null hypothesis (false negative)\n\n![](../images/lecture2/p_value.png)\n\n![](../images/lecture2/null_hypothesis.png){.absolute .fragment .fade-in bottom=\"400\" left=\"150\" width=\"25%\"}\n\n![](../images/lecture2/alt_hypothesis.png){.absolute .fragment .fade-in bottom=\"400\" left=\"450\" width=\"25%\"}\n\n![](../images/lecture2/blue_wedge.png){.absolute .fragment .fade-in bottom=\"120\" left=\"460\" width=\"10%\"}\n\n![](../images/lecture2/red_wedge.png){.absolute .fragment .fade-in bottom=\"120\" left=\"250\" width=\"20%\"}\n\n![](../images/lecture2/sample_size.png){.absolute .fragment .fade-in bottom=\"300\" left=\"300\" width=\"20%\"}\n\n![](../images/lecture2/red_arrow.png){.absolute .fragment .fade-in bottom=\"250\" left=\"300\" width=\"20%\"}\n\n## Poll\n\n::: {style=\"position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;\"}\n<iframe sandbox=\"allow-scripts allow-same-origin allow-presentation\" allowfullscreen=\"true\" allowtransparency=\"true\" frameborder=\"0\" height=\"315\" src=\"https://www.mentimeter.com/app/presentation/alf9arqzebexh9jdrkry69dwkgd8nydp/embed\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" width=\"420\">\n\n</iframe>\n:::\n\n## Problems with p-values {.smaller}\n\nAnswer: Depends but general safe answer...NO!\n\n-   E.g., A change in the gas price will produce some changes in travel behavior. How do these changes vary across people and contexts?\n-   A medical intervention will work differently for different people\n-   A political advertisement will change the opinion of some people but not others\n-   In many cases, there is no interest in a null effect\n-   When a hypothesis is rejected (i.e., a study is a success), researchers and practitioners make decisions based on point estimates of the magnitude and sign\n    -   We are interested in the magnitude (M) and sign (S) conditional on an effect being statistically significant\n\n## Statistically Significance $\\neq$ Practical Importance {.smaller}\n\n::: incremental\n-   An effect can be non-random (i.e., statistically significant) but not practically impactful\n-   E.g., say we find that for every 10-cent increase in the price per gallon for gasoline there is a 0.001% decrease in vehicle miles traveled. Do we care?\n    -   Approximate actual elasticity values: -0.0216 (short-run) and -0.1066 (long-run)\n    -   Meaning a 1% increase in the price of gas produces a 0.026% decrease in VMT\n    -   If gas price rises from \\$3.00 per gallon to \\$3.10 per gallon\n        -   3.3% increase in price -\\> 0.09% short-run decrease in VMT\n        -   If we double the price -\\> 2.2% short-run decrease in VMT OR 10.7% in long-run\n:::\n\n## Not Significant Does Not Mean Zero Effect {.smaller}\n\n::: incremental\n-   E.g., A study of the effectiveness of heart stents for heart patients\n-   Treated group outperformed control group, but not statistically significantly so\n-   Observed average difference in treadmill time (treatment effect) was 16.6 seconds with standard error of 9.8 ‚Äì 95% confidence interval includes zero\n-   Net effect may be positive or negative ‚Äì it is unclear!\n:::\n\n## Differences B/W *Significant* & *Not Significant* NOT Statistically Significant {.smaller}\n\n::: incremental\n-   Moving from 0.051 p-value to 0.049 p-value is not hard\n-   More important:\n    -   Large differences in significance level may not mean large differences in underlying variable\n    -   E.g., Two independent studies with effect estimates and standard errors of 25 ¬± 10 and 10 ¬± 10\n        -   First study is significant at 0.01 level (25/10 = 2.5)\n        -   Second study is not significant (10/10 = 1.0)\n        -   Is there a large difference between the two effect estimates?\n        -   Difference is 15 with a standard error of $\\sqrt{(10^2+10^2)}=14$ meaning 15/14 = 1.07!\n:::\n\n## Garden of Forking Paths {.smaller}\n\n-   When many ways to select, exclude, & analyze data, not difficult to attain a low p-value (even in absence of true effect)\n-   More than ‚Äúfile drawer effect‚Äù ‚Äì not publishing non-significant results\n-   ‚ÄúDegrees of freedom‚Äù available to analyst when coding & analyzing data\n-   Even if only one analysis done, there are many others that could be done that would result in non-significant results\n\n![](../images/lecture2/forking_fish.jpg){fig-align=\"center\"}\n\n```{=html}\n<!-- Neuroscientist Craig Bennett purchased a whole Atlantic salmon, took it to a lab at Dartmouth, and put it into an fMRI machine used to study the brain. The beautiful fish was to be the lab's test object as they worked out some new methods.\n\nSo, as the fish sat in the scanner, they showed it \"a series of photographs depicting human individuals in social situations.\" To maintain the rigor of the protocol (and perhaps because it was hilarious), the salmon, just like a human test subject, \"was asked to determine what emotion the individual in the photo must have been experiencing.‚Äú\n\nThe salmon, as¬†Bennett's poster on the test dryly notes, \"was not alive at the time of scanning.\"\n\nanalyzing the voxel (think: 3-D or \"volumetric\" pixel) data, the voxels representing the area where the salmon's tiny brain sat showed evidence of activity. In the fMRI scan, it looked like the dead salmon was¬†actually thinking¬†about the pictures it had been shown.\n\n\"By complete, random chance, we found some voxels that were significant that just happened to be in the fish's brain,\" Bennett said. \"And if I were a ridiculous researcher, I'd say, 'A dead salmon perceiving humans can tell their emotional state.'\" -->\n```\n\n## Example: Sports Viewing & Political Attitudes {.smaller}\n\nConsider the below article excerpt (based on an article in a leading psychology journal):\n\nMany people watch UNL Cornhusker football games. Whereas research finds that Cornhuskers viewing influences Nebraskans‚Äô mating preferences, we propose that it might also change their political and religious views. Building on theory suggesting that political and religious orientation are linked to viewing preference, we test how football season influenced Nebraskans‚Äô politics, religiosity, and voting in the 2012 U.S. presidential election. In two studies with large and diverse samples, whether it was football season had drastically different effects on single versus married Nebraskans. Football season led single Nebraskans to become more liberal, less religious, and more likely to vote for Barack Obama. In contrast, football led married Nebraskans to become more conservative, more religious, and more likely to vote for Mitt Romney. In addition, football-induced changes in political orientation mediated Nebraskans‚Äô voting behavior. Overall, the football season not only influences Nebraskans‚Äô politics, but appears to do so differently for single versus married Nebraskans.\n\n## Example: Sports Viewing & Political Attitudes\n\n::: incremental\n-   Find **40%** of Nebraskans who watch football supported Romney in football season vs. **23%** in non-football season (Type-M error)\n\n-   Implausible! Research finds **minimal changes** in vote preferences over election cycle\n\n-   What is the dividing line between **single** and **married**?\n\n-   **Differential response rate?** Maybe liberal or conservative Nebraskans are more or less likely to participate in a survey depending on if it is football season\n:::\n\n## How to Move Beyond Hypothesis Testing\n\n::: incremental\n1.  **Analyze all data** - Better to anticipate critism than hide data\n2.  **Present all comparisons** - Rather than statistically significant comparisons only\n3.  **Make data public** - If a topic is worth studying, you should want others to be able to quickly progress without repeating your work\n4.  **Accept uncertainty & embrace variation** - No answer is perfect\n:::"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"lec-2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.5.56","auto-stretch":true,"editor":"visual","title":"Lecture 2 - Statistical Modeling","subtitle":"CIVE 461/861: Urban Transportation Planning","footer":"[CIVE461 Home](/teaching/CIVE461/)","logo":"../images/logo.png","theme":"simple","transition":"fade","chalkboard":true,"slideNumber":true,"multiplex":true}}},"projectFormats":["html"]}